{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: 分割数据和指令\n",
    "\n",
    "- [Lesson](#lesson)\n",
    "- [Exercises](#exercises)\n",
    "- [Example Playground](#example-playground)\n",
    "\n",
    "\n",
    "## 设置\n",
    "运行以下设置单元格以加载您的 API 密钥并建立 get_completion 辅助函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anthropic in /opt/miniconda3/lib/python3.13/site-packages (0.56.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/miniconda3/lib/python3.13/site-packages (from anthropic) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/miniconda3/lib/python3.13/site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /opt/miniconda3/lib/python3.13/site-packages (from anthropic) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/miniconda3/lib/python3.13/site-packages (from anthropic) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/miniconda3/lib/python3.13/site-packages (from anthropic) (2.10.3)\n",
      "Requirement already satisfied: sniffio in /opt/miniconda3/lib/python3.13/site-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /opt/miniconda3/lib/python3.13/site-packages (from anthropic) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/miniconda3/lib/python3.13/site-packages (from anyio<5,>=3.5.0->anthropic) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/miniconda3/lib/python3.13/site-packages (from httpx<1,>=0.25.0->anthropic) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/lib/python3.13/site-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/miniconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda3/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/miniconda3/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (2.27.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install anthropic\n",
    "\n",
    "# Import python's built-in regular expression library\n",
    "import re\n",
    "import anthropic\n",
    "\n",
    "# Retrieve the API_KEY & MODEL_NAME variables from the IPython store\n",
    "%store -r API_KEY\n",
    "%store -r MODEL_NAME\n",
    "\n",
    "def get_completion(prompt: str, system_prompt=\"\"):\n",
    "    client = anthropic.Anthropic(api_key=API_KEY)\n",
    "    message = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.0,\n",
    "        system=system_prompt,\n",
    "        messages=[\n",
    "          {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## 课程\n",
    "通常情况下，我们不想写完整的提示词，而是想要可以在提交给LLM之前用额外的输入数据进行修改的提示模板。如果您希望 LLM每次都做同样的事情，但 Claude 用于其任务的数据可能每次都不同，这可能会派上用场。\n",
    "幸运的是，我们可以通过将提示的固定框架与可变用户输入分离，然后在将完整提示发送给 Claude 之前将用户输入替换到提示中来轻松实现这一点。\n",
    "下面，我们将逐步介绍如何编写可替换的提示模板，以及如何替换用户输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 示例\n",
    "在第一个示例中，我们要求 LLM 充当动物声音生成器。请注意，提交给 LLM 的完整提示只是用输入（在这种情况下是\"Cow\"）替换的 PROMPT_TEMPLATE。请注意，当我们打印完整提示时，\"Cow\"这个词通过 f-string 替换了 ANIMAL 占位符。\n",
    "注意： 在实践中，您不必将占位符变量命名为特定的名称。我们在这个示例中将其称为 ANIMAL，但同样容易地，我们也可以将其称为 CREATURE 或 A（尽管通常最好让您的变量名称具体且相关，这样即使没有替换，您的提示模板也易于理解，只是为了用户的可解析性）。只需确保您为变量命名的任何名称都是您在提示模板 f-string 中使用的名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------- Full prompt with variable substutions ---------------------------\n",
      "我会告诉您一个动物的名字。请用这个动物发出的声音来回应。. 猪\n",
      "\n",
      "------------------------------------- Claude's response -------------------------------------\n",
      "呼噜呼噜~\n"
     ]
    }
   ],
   "source": [
    "# Variable content\n",
    "动物 = \"猪\"\n",
    "\n",
    "# Prompt template with a placeholder for the variable content\n",
    "PROMPT = f\"我会告诉您一个动物的名字。请用这个动物发出的声音来回应。. {动物}\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(\"--------------------------- Full prompt with variable substutions ---------------------------\")\n",
    "print(PROMPT)\n",
    "print(\"\\n------------------------------------- Claude's response -------------------------------------\")\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "为什么我们要像这样分离和替换输入呢？嗯，**提示模板简化了重复性任务**。假设您构建了一个提示结构，邀请第三方用户向提示提交内容（在这种情况下是他们想要生成声音的动物）。这些第三方用户不必编写甚至看到完整的提示。他们所要做的就是填写变量。\n",
    "我们在这里使用变量和 f-string 进行这种替换，但您也可以使用 format() 方法来实现。\n",
    "**注意： 提示模板可以有任意数量的变量！**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "当像这样引入替换变量时，确保 LLM **知道变量在哪里开始和结束（相对于指令或任务描述）非常重要**。让我们看一个指令和替换变量之间没有分离的示例。\n",
    "对我们人类的眼睛来说，下面提示模板中变量的开始和结束位置非常清楚。然而，在完全替换的提示中，这种界限变得不明确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------- Full prompt with variable substutions ---------------------------\n",
      "You Jack. 我要求你明天早晨6点到达，因为我是公司CEO. <----- 让EMAL的内容更加礼貌，但不要改变其他任何内容.\n",
      "\n",
      "------------------------------------- LLM's response -------------------------------------\n",
      "亲爱的CEO,\n",
      "\n",
      "我很高兴能为公司效劳。我会尽力在明天早上6点准时到达。我理解您作为公司领导的重要性,我会全力配合您的要求。\n",
      "\n",
      "如果有任何需要我协助的地方,请随时告诉我。我会竭尽全力完成任务,为公司做出应有的贡献。\n",
      "\n",
      "再次感谢您的信任,我会在明天早上准时到达。\n",
      "\n",
      "祝工作顺利!\n"
     ]
    }
   ],
   "source": [
    "# Variable content\n",
    "EMAIL = \"我要求你明天早晨6点到达，因为我是公司CEO.\"\n",
    "\n",
    "# Prompt template with a placeholder for the variable content\n",
    "PROMPT = f\"You Jack. {EMAIL} <----- 让EMAL的内容更加礼貌，但不要改变其他任何内容.\"\n",
    "\n",
    "\n",
    "# Print Claude's response\n",
    "print(\"--------------------------- Full prompt with variable substutions ---------------------------\")\n",
    "print(PROMPT)\n",
    "print(\"\\n------------------------------------- LLM's response -------------------------------------\")\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, **Claude thinks \"Yo Claude\" is part of the email it's supposed to rewrite**! You can tell because it begins its rewrite with \"Dear Claude\". To the human eye, it's clear, particularly in the prompt template where the email begins and ends, but it becomes much less clear in the prompt after substitution.\n",
    "\n",
    "在这里，**LLM 认为\"Yo Claude\"是它应该重写的邮件的一部分！** 您可以通过它用\"Dear Claude\"开始重写来看出这一点。对人类的眼睛来说，特别是在提示模板中，邮件的开始和结束位置很清楚，但在替换后的提示中变得不那么清楚了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "我们如何解决这个问题？**用 XML 标签包装输入**！我们在下面这样做了，正如您所看到的，输出中不再有\"Dear Claude\"。\n",
    "XML 标签是像 <tag></tag> 这样的尖括号标签。它们成对出现，由开始标签（如 <tag>）和由 / 标记的结束标签（如 </tag>）组成。XML 标签用于包装内容，像这样：<tag>content</tag>。\n",
    "注意： 虽然 Claude 可以识别和使用各种分隔符和定界符，但我们建议您专门使用 XML 标签作为 LLM 的分隔符，因为 LLM 经过专门训练，将 XML 标签识别为提示组织机制。除了函数调用之外，没有 LLM 经过训练的特殊 XML 标签可以让您最大限度地提升性能。我们有意让 LLM 以这种方式变得非常灵活和可定制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable content\n",
    "EMAL = \"我要求你明天早晨6点到达，因为我是公司CEO.\"\n",
    "\n",
    "# Prompt template with a placeholder for the variable content\n",
    "PROMPT = f\"You Jack. <EMAL> {EMAL} </EMAL>  <-----让EMAIL的内容更加礼貌，但不要改变其他任何内容.\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(\"--------------------------- Full prompt with variable substutions ---------------------------\")\n",
    "print(PROMPT)\n",
    "print(\"\\n------------------------------------- Claude's response -------------------------------------\")\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "让我们看看 XML 标签如何帮助我们的另一个示例。\n",
    "在以下提示中，**LLM错误地解释了提示的哪一部分是指令与输入**。由于格式的原因，它错误地将 Each is about an animal, like rabbits 视为列表的一部分，而用户（填写 SENTENCES 变量的人）可能不希望这样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable content\n",
    "\n",
    "SENTENCES = \"\"\"- 我喜欢牛的叫声\n",
    "- 这句话是关于蜘蛛的\n",
    "- 这句话看起来是关于狗的，但实际上是关于猪的\"\"\"\n",
    "\n",
    "# Prompt template with a placeholder for the variable content\n",
    "\n",
    "PROMPT = f\"\"\"下面是一个句子列表。告诉我列表中的第二项。\n",
    "\n",
    "-每个都是关于动物的，比如兔子。\n",
    "{SENTENCES}\"\"\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(\"--------------------------- Full prompt with variable substutions ---------------------------\")\n",
    "print(PROMPT)\n",
    "print(\"\\n------------------------------------- Claude's response -------------------------------------\")\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要修复这个问题，我们只需要用 **XML 标签包围用户输入的句子**。这向 LLM 显示了输入数据的开始和结束位置，尽管在 Each is about an animal, like rabbits. 之前有误导性的连字符`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------- Full prompt with variable substutions ---------------------------\n",
      "下面是一个句子列表。告诉我列表中的第二项。\n",
      "-每个都是关于动物的，比如兔子。\n",
      "\n",
      "<sentences>\n",
      "- 我喜欢牛的叫声\n",
      "- 这句话是关于蜘蛛的\n",
      "- 这句话看起来是关于狗的，但实际上是关于猪的\n",
      "</sentences>\n",
      "\n",
      "------------------------------------- Claude's response -------------------------------------\n",
      "这句话是关于蜘蛛的。\n"
     ]
    }
   ],
   "source": [
    "# Variable content\n",
    "SENTENCES = \"\"\"- I like how cows sound\n",
    "- This sentence is about spiders\n",
    "- This sentence may appear to be about dogs but it's actually about pigs\"\"\"\n",
    "\n",
    "\n",
    "SENTENCES = \"\"\"- 我喜欢牛的叫声\n",
    "- 这句话是关于蜘蛛的\n",
    "- 这句话看起来是关于狗的，但实际上是关于猪的\"\"\"\n",
    "\n",
    "# Prompt template with a placeholder for the variable content\n",
    "\n",
    "PROMPT = f\"\"\"下面是一个句子列表。告诉我列表中的第二项。\n",
    "-每个都是关于动物的，比如兔子。\n",
    "\n",
    "<sentences>\n",
    "{SENTENCES}\n",
    "</sentences>\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Print Claude's response\n",
    "print(\"--------------------------- Full prompt with variable substutions ---------------------------\")\n",
    "print(PROMPT)\n",
    "print(\"\\n------------------------------------- Claude's response -------------------------------------\")\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意： 在\"Each is about an animal\"提示的错误版本中，我们必须包含连字符才能让 LLM 以我们为这个示例想要的方式错误地回应。这是关于提示工程的重要教训：**小细节很重要！** ，**清理提示中的拼写错误和语法错误总是值得的**。LLM 对模式很敏感（在早期，在微调之前，它是一个原始的文本预测工具），当您犯错时它更可能犯错，当您听起来聪明时它更聪明，当您听起来愚蠢时它更愚蠢，等等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 练习\n",
    "- [Exercise 4.1 - Haiku Topic](#exercise-41---haiku-topic)\n",
    "- [Exercise 4.2 - Dog Question with Typos](#exercise-42---dog-question-with-typos)\n",
    "- [Exercise 4.3 - Dog Question Part 2](#exercise-42---dog-question-part-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 练习 4.1 - 诗歌主题\n",
    "修改 PROMPT，使其成为一个模板，该模板将接收一个名为 TOPIC 的变量，并输出关于该主题的诗歌。这个练习只是为了测试您对使用 f-string 的变量模板结构的理解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable content\n",
    "TOPIC = \"Pigs\"\n",
    "\n",
    "# Prompt template with a placeholder for the variable content\n",
    "PROMPT = f\"\"\n",
    "\n",
    "# Get Claude's response\n",
    "response = get_completion(PROMPT)\n",
    "\n",
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    return bool(re.search(\"pigs\", text.lower()) and re.search(\"诗歌\", text.lower()))\n",
    "\n",
    "# Print Claude's response\n",
    "print(\"--------------------------- Full prompt with variable substutions ---------------------------\")\n",
    "print(PROMPT)\n",
    "print(\"\\n------------------------------------- Claude's response -------------------------------------\")\n",
    "print(response)\n",
    "print(\"\\n------------------------------------------ GRADING ------------------------------------------\")\n",
    "print(\"This exercise has been correctly solved:\", grade_exercise(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ If you want a hint, run the cell below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 练习 4.2 - 带有拼写错误的狗问题\n",
    "通过添加 XML 标签来修复 PROMPT，使 LLM 产生正确的答案。\n",
    "尽量不要改变提示的其他任何内容。混乱和错误百出的写作是故意的，这样您可以看到 LLM 对这些错误的反应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable content\n",
    "QUESTION = \"ar cn brown?\"\n",
    "\n",
    "# Prompt template with a placeholder for the variable content\n",
    "PROMPT = f\"Hia its me i have a q about dogs jkaerjv {QUESTION} jklmvca tx it help me muhch much atx fst fst answer short short tx\"\n",
    "\n",
    "# Get Claude's response\n",
    "response = get_completion(PROMPT)\n",
    "\n",
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    return bool(re.search(\"brown\", text.lower()))\n",
    "\n",
    "# Print Claude's response\n",
    "print(\"--------------------------- Full prompt with variable substutions ---------------------------\")\n",
    "print(PROMPT)\n",
    "print(\"\\n------------------------------------- Claude's response -------------------------------------\")\n",
    "print(response)\n",
    "print(\"\\n------------------------------------------ GRADING ------------------------------------------\")\n",
    "print(\"This exercise has been correctly solved:\", grade_exercise(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 练习 4.3 - 狗问题第二部分\n",
    "\n",
    "修复 PROMPT，不要添加 XML 标签。相反，只从提示中删除一两个单词。\n",
    "与上述练习一样，尽量不要改变提示的其他任何内容。这将向您展示 LLM 能够解析和理解什么样的语言。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------- Full prompt with variable substutions ---------------------------\n",
      "Hia its me i have a q about dogs jkaerjv ar cn brown? jklmvca tx it help me muhch much atx fst fst answer short short tx\n",
      "\n",
      "------------------------------------- Claude's response -------------------------------------\n",
      "I'm afraid I don't fully understand your question. Could you please rephrase it in a more clear and concise way? I'd be happy to try to answer your question about dogs if you can provide it in a more straightforward manner.\n",
      "\n",
      "------------------------------------------ GRADING ------------------------------------------\n",
      "This exercise has been correctly solved: False\n"
     ]
    }
   ],
   "source": [
    "# Variable content\n",
    "QUESTION = \"ar cn brown?\"\n",
    "\n",
    "# Prompt template with a placeholder for the variable content\n",
    "PROMPT = f\"Hia its me i have a q about dogs jkaerjv {QUESTION} jklmvca tx it help me muhch much atx fst fst answer short short tx\"\n",
    "\n",
    "# Get Claude's response\n",
    "response = get_completion(PROMPT)\n",
    "\n",
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    return bool(re.search(\"brown\", text.lower()))\n",
    "\n",
    "# Print Claude's response\n",
    "print(\"--------------------------- Full prompt with variable substutions ---------------------------\")\n",
    "print(PROMPT)\n",
    "print(\"\\n------------------------------------- Claude's response -------------------------------------\")\n",
    "print(response)\n",
    "print(\"\\n------------------------------------------ GRADING ------------------------------------------\")\n",
    "print(\"This exercise has been correctly solved:\", grade_exercise(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 恭喜！\n",
    "如果您已经解决了到此为止的所有练习，您就可以进入下一章了。祝您提示词工程愉快！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
